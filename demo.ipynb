{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTqAxOud9Z_a"
      },
      "source": [
        "# Experiment Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aIZcIFsB9Z_b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os.path as osp\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "import argparse\n",
        "\n",
        "\n",
        "class BaseDataset(Dataset):\n",
        "    def __init__(self, file_path, split, max_length=256):  \n",
        "        self.max_length = max_length  # the maximum number of tokens in the sequence\n",
        "        self.split = split  # from inherited classes\n",
        "\n",
        "        if split == 'test':\n",
        "            # \"Note that the test data will follow the same format as the files in the trial dataset, except that the label column will not be provided\"\n",
        "            df = pd.read_csv(osp.join(file_path, 'test.csv'), dtype={'text_1':str, 'text_2': str, 'label': int})  # get data in specific types\n",
        "        else:\n",
        "            df = pd.read_csv(osp.join(file_path, split + '.csv'), dtype={'text_1':str, 'text_2': str, 'label': int})\n",
        "        text1 = list(df['text_1'])  # text1\n",
        "        text2 = list(df['text_2'])  # text2\n",
        "\n",
        "        if split != 'test':\n",
        "            label = list(df['label'])\n",
        "\n",
        "        self.data = []\n",
        "        for i in range(len(text1)):\n",
        "            if type(text1[i]) != str or type(text2[i]) != str:\n",
        "                text1[i] = text2[i] =  ''\n",
        "                bad_data = True  # null test sequences\n",
        "            elif len(text1[i]) < 5 or len(text2[i]) < 5:\n",
        "                bad_data = True  # short sequences\n",
        "            else:\n",
        "                bad_data = False  # normal sequences\n",
        "\n",
        "            self.data.append({\n",
        "                'text1': text1[i],\n",
        "                'text2': text2[i],\n",
        "                'bad_data': bad_data,\n",
        "            })\n",
        "\n",
        "            if split != 'test':\n",
        "                self.data[-1]['label'] = label[i]\n",
        "\n",
        "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "        self.unk_token_idx = self.tokenizer.convert_tokens_to_ids(self.tokenizer.unk_token)\n",
        "        self.sep_token_idx = self.tokenizer.convert_tokens_to_ids(self.tokenizer.sep_token)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # return length\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        tokens = self.tokenizer(text, padding='max_length', max_length=self.max_length, truncation=True)  # setting the tokenizer\n",
        "        input_ids = tokens['input_ids']  # gain a series of index values of tokens, after tokenizing sequences\n",
        "        input_ids = self.aug(input_ids)  # data augmentation\n",
        "        input_ids = torch.tensor(input_ids)  # convert into the tensor format that PyTorch could understand\n",
        "\n",
        "        return input_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get data by random idx generated by pytorch itself.\n",
        "        token_idx_1 = self.preprocess(self.data[idx]['text1'])\n",
        "        token_idx_2 = self.preprocess(self.data[idx]['text2'])\n",
        "        bad_data = self.data[idx]['bad_data']\n",
        "\n",
        "        if self.split == 'test':\n",
        "            return token_idx_1, token_idx_2, bad_data  # no label column, if it was the test mode\n",
        "\n",
        "        label = self.data[idx]['label']\n",
        "        return token_idx_1, token_idx_2, label\n",
        "\n",
        "\n",
        "class TestDataset(BaseDataset):\n",
        "    def __init__(self, split, *args, **kwargs):\n",
        "        assert split in ['dev', 'test']  # mainly for testing\n",
        "        super(TestDataset, self).__init__(split=split, *args, **kwargs)\n",
        "\n",
        "    def aug(self, input_ids, p=0.5):\n",
        "        return input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3j4xB5s9Z_c"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5xqMmjjJ9Z_c"
      },
      "outputs": [],
      "source": [
        "class BertVerifier(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size=768, dropout=0.0):\n",
        "        super(BertVerifier, self).__init__()\n",
        "\n",
        "        self.encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.pooling_fn = self.pooling_fn_cls_token\n",
        "        for p in self.encoder.embeddings.parameters():\n",
        "            p.requires_grad = False  # froze the word embedding to prevent overfitting\n",
        "        for m in self.encoder.transformer.layer[:3]:\n",
        "            # low-level layers would pay more attention to the basic linguistics knowledge, from \"https://aclanthology.org/N18-1202/\"\n",
        "            for p in m.parameters():  # froze parameters of the first three layers to prevent overfitting\n",
        "                p.requires_grad = False  # not enough training data\n",
        "\n",
        "        self.fc1 = nn.Linear(768, hidden_size)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def pooling_fn_cls_token(self, outputs):\n",
        "        # extract \"sequence of hidden-states at the output of the last layer of the model\"\n",
        "        # https://huggingface.co/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel\n",
        "        return outputs[0][:, 0]\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Forward\n",
        "        outputs = self.encoder(torch.cat([x1, x2], dim=0).long())  # (M,a) (M,a) -> (M+M, a): two paragraphs were concatenated along the batch direction\n",
        "        emb = self.pooling_fn(outputs)\n",
        "\n",
        "        # prediction head\n",
        "        emb = self.dropout(emb)\n",
        "        emb = self.fc1(emb)\n",
        "        emb = self.bn1(emb)\n",
        "        emb = F.gelu(emb)\n",
        "        emb = self.fc2(emb)\n",
        "        emb = F.normalize(emb, p=2, dim=1)  # normalisation for the cosine similarity\n",
        "\n",
        "        emb1, emb2 = emb.chunk(2)  # waist intercept, above was text1 and below was text2\n",
        "        sim = (emb1 * emb2).sum(1)  # the cosine similarity dot product after normalization\n",
        "\n",
        "        return sim\n",
        "\n",
        "\n",
        "class LSTMVerifier(nn.Module):\n",
        "    def __init__(self, output_size, hidden_size=768, num_layers=3, dropout=0.0):\n",
        "        super(LSTMVerifier, self).__init__()\n",
        "        # extract the embedding from DistilBERT tokenization\n",
        "        self.emb_layer = DistilBertModel.from_pretrained('distilbert-base-uncased').embeddings\n",
        "        self.encoder = nn.LSTM(768, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)  # bi-LSTM\n",
        "\n",
        "        self.fc1 = nn.Linear(1536, hidden_size)  # 1536=768*2\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        # Forward\n",
        "        inputs = torch.cat([x1, x2], dim=0).long() # index values of tokens\n",
        "        inputs = self.emb_layer(inputs)  # the embedding layer with \"Deep contextualized word representations\" from \"https://aclanthology.org/N18-1202/\"\n",
        "        outputs, _ = self.encoder(inputs)\n",
        "        # average features over the sequential dimension to serve as feature representations of the whole sequence\n",
        "        emb = outputs.mean(1)\n",
        "\n",
        "        # prediction head similar to DistilBERT, BUT WITHOUT dropout layer\n",
        "        emb = self.fc1(emb)\n",
        "        emb = self.bn1(emb)\n",
        "        emb = F.gelu(emb)\n",
        "        emb = self.fc2(emb)\n",
        "        emb = F.normalize(emb, p=2, dim=1)\n",
        "\n",
        "        emb1, emb2 = emb.chunk(2)\n",
        "        sim = (emb1 * emb2).sum(1)\n",
        "\n",
        "        return sim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as2bv9BG9Z_c"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJzy0t1r9Z_c",
        "outputId": "b3b9c2d5-a454-416b-e83c-f470ba6aae04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/375 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
            "100%|██████████| 375/375 [00:57<00:00,  6.55it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "@torch.no_grad()\n",
        "def inference(args):\n",
        "\n",
        "    # init model\n",
        "    if args.model_type == 'distilbert':\n",
        "        model = BertVerifier(args.emb_size, args.hidden_size)\n",
        "        pretrained_path = '/content/drive/MyDrive/save/distilbert/best_weights.pth'\n",
        "    elif args.model_type == 'lstm':\n",
        "        model = LSTMVerifier(args.emb_size, args.hidden_size, args.num_layers)\n",
        "        pretrained_path = '/content/drive/MyDrive/save/lstm/best_weights.pth'\n",
        "    else:\n",
        "        raise NotImplementedError(\"Model type not implemented.\")\n",
        "\n",
        "    model.load_state_dict(torch.load(pretrained_path, map_location='cpu'))\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    # load data\n",
        "    test_set = TestDataset('test', args.data_path, max_length=args.max_length)\n",
        "    test_loader = DataLoader(test_set, batch_size=args.bs, pin_memory=True, shuffle=False, drop_last=False)\n",
        "\n",
        "    preds_all = []\n",
        "    for text1, text2, bad_data in tqdm(test_loader):\n",
        "        scores = model(text1.cuda(), text2.cuda())\n",
        "        preds = (scores > 0.18).long().data.cpu()\n",
        "        preds *= 1 - bad_data.long()  # all invalid sequence pairs were viewed as 0\n",
        "        preds_all.append(preds)\n",
        "\n",
        "    preds_all = torch.cat(preds_all).numpy()\n",
        "\n",
        "    if not os.path.exists('/'.join(args.output_path.split('/')[:-1])):\n",
        "        os.makedirs('/'.join(args.output_path.split('/')[:-1]))\n",
        "    with open(args.output_path, 'w') as f:\n",
        "        f.write(\"prediction\\n\")\n",
        "        out = [str(i) for i in preds_all]\n",
        "        f.write('\\n'.join(out))\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()  # parse some default parameters passed in, from the command line\n",
        "    parser.add_argument(\"--hidden_size\", type=int, default=768, help=\"embedding dimensions of hidden layers\")\n",
        "    parser.add_argument(\"--emb_size\", type=int, default=256)\n",
        "    parser.add_argument(\"--num_layers\", type=int, default=3)\n",
        "    parser.add_argument(\"--bs\", type=int, default=16, help=\"batch size per gpu\")\n",
        "    parser.add_argument(\"--max_length\", type=int, default=256, help=\"max length of tokens\")\n",
        "    parser.add_argument(\"--model_type\", type=str, choices=['distilbert', 'lstm'], default='distilbert')\n",
        "    parser.add_argument(\"--data_path\", type=str, default='./test_data/AV')\n",
        "    parser.add_argument(\"--output_path\", type=str, default='./outputs/distilbert/Group_52_C.csv')\n",
        "\n",
        "    args, unknown = parser.parse_known_args()  # get the namespace of all command line arguments, just like a python object\n",
        "\n",
        "    inference(args)   # execute the experiment :)\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "interpreter": {
      "hash": "9c7b29a3529c140420126446c3d126c7a36d0833188f5baf93b843fa26825432"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
